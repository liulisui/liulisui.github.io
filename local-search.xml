<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>gpt-day-6</title>
    <link href="/2024/05/27/gpt-day-6/"/>
    <url>/2024/05/27/gpt-day-6/</url>
    
    <content type="html"><![CDATA[<p>1、Multi-Head Attention:</p>]]></content>
    
    
    
    <tags>
      
      <tag>make gpt from scratch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>gpt-day-5</title>
    <link href="/2024/05/26/gpt-day-5/"/>
    <url>/2024/05/26/gpt-day-5/</url>
    
    <content type="html"><![CDATA[<p>1、torch.randint(low&#x3D;a,high&#x3D;b(取不到)，size&#x3D;(张量形状))</p><p>该函数用于对某一形状的张量填充随机数字</p><p>2、attention:在过去的课程中，我们对每一个token的embedding只是做了简单的平均，但事实上为了实现更复杂的效果，每一个embedding的权重应该不同。</p><p><img src="/./img/gpt-day-5-1.png"></p><p>如图，在左边的这个T*T矩阵中，(i,j)元素可以理解为第i个token和第j个token之间的strength&#x2F;score&#x2F;affinity，本质上是，在已经考虑了i-1个token,尝试加入第i个token时，这个第i个token和前面的各个token之间的联系。</p><p>而self attention layer的目的，就是为任意的输入生成适合的权重。</p><p>attention公式如图：</p><p><img src="/./img/gpt-day-5-2.png"></p><p>其中的Q是由每个token的embedding经过linear层生成的，用于每个token去匹配别的token的向量，而K也是由每个token的embedding经过linear层生成的，用于被其他token匹配的向量，两者的维度相同，均为T*attention_dim，而Q点乘K转置会生成T×T矩阵，矩阵中第(i,j)个元素的值越大，可以理解为第i个token向第j个token的匹配程度越高。</p><p><img src="/./img/gpt-day-5-3.png"></p><p>softmax函数本质是将一行数据中全部压缩到0与1之间，并且这一行数据的和为0，这是对数据的优化处理。</p><p>dk就是attention_dim,它只是一个常数，但是除以根号dk能够避免一些梯度爆炸问题（算出来的值非常大）。</p><p>这样我们就得到了scores矩阵，表示每个token两两之间的关系。</p><p>而v,也就是由embedding生成的value，表示了每个token要传达的信息，将scores点乘v，我们就能得到在加入相应token之后的embedding。</p><p>3、在tensor运算中，@符号可视为torch.matmul,即为矩阵点乘</p><p>4、为了使得未来的token不会对当下的token产生影响，使用mask掩码方法，也就是把那些位置全部赋值为负无穷，这样e的负无穷次可以视为0，不会对整体权重产生影响。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">tri=torch<span class="hljs-selector-class">.tril</span>(torch<span class="hljs-selector-class">.ones</span>(key<span class="hljs-selector-class">.shape</span><span class="hljs-selector-attr">[1]</span>,key<span class="hljs-selector-class">.shape</span><span class="hljs-selector-attr">[1]</span>))<span class="hljs-selector-id">#key</span><span class="hljs-selector-class">.shape</span><span class="hljs-selector-attr">[1]</span>也就是T,文本长度<br><br>scores=scores<span class="hljs-selector-class">.masked_fill</span>(tri==<span class="hljs-number">0</span>,<span class="hljs-attribute">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>))<br><br>scores=torch<span class="hljs-selector-class">.nn</span><span class="hljs-selector-class">.functional</span><span class="hljs-selector-class">.softmax</span>(scores,dim=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>make gpt from scratch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>gpt-day-4</title>
    <link href="/2024/05/25/gpt-day-4/"/>
    <url>/2024/05/25/gpt-day-4/</url>
    
    <content type="html"><![CDATA[<p>1、one-hot-encoding（热编码）：<br>    <img src="/./img/gpt-day-4-1.png"></p> <p style="text-indent: 2em;"> 一个string个数*总词汇个数的矩阵，每一行中有一个1，每行表示1的列坐标对应的string。 <p style="text-indent: 2em;">事实上，lookup table是一个总词汇个数*embedding维度的矩阵，如果我们把one-hot-input和lookup table矩阵相乘，则得到的矩阵的每一行都是对应string的embedding<p><img src="/./img/gpt-day-4-2.png"></p><p>2、nn.embedding可以理解为由nn.linear(总词汇数，embedding维度)生成的</p><p><img src="/./img/gpt-day-4-3.png"></p><p>3、总结：</p><p><img src="/./img/gpt-day-4-4.png"></p><p>图中的avg可以理解为由每个单词的意思整合为一句话的意思(NLP中的bag of words模型），最终我们要得到的是每句话的积极和消极程度</p>]]></content>
    
    
    
    <tags>
      
      <tag>make gpt from scratch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>gpt-day-3</title>
    <link href="/2024/05/23/gpt-day-3/"/>
    <url>/2024/05/23/gpt-day-3/</url>
    
    <content type="html"><![CDATA[<p>1、ReLU激活函数&#x3D;max(0,x)</p><p>​激活函数Sigmoid(x)&#x3D;1&#x2F;(1+e*(-x)),产生0-1之间的数字</p><p>2、激活函数和dropout像是包在每个神经元外的一层膜，它们会对神经元想要传达的数据作一定的处理。</p><p>3、训练模型</p><p>1）定义一个模型对象</p><p>2）定义一个误差计算函数（在之前的回归计算，希望用模型和几个参数来计算一个值，比如已知父母身高和当前体重身高预测最终身高，用到的是mse，但是这里识别图片中的数字，希望得到的是每个数字的概率，有点像gpt中每个token的概率，二者用到的是nn.CrossEntropyLoss()函数）</p><p>3）优化器：optimizer&#x3D;torch.optim.Adam(model.parameters())</p><p>其中Adam的本质是梯度下降法，但是它的learning rate是可以调节的(比如马上要最低点了，能把learning rate调低，就不会调节过头)</p><p>4）循环训练：</p><p>​a、前向传播，得到模型预测值</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">model_prediction</span> = model(images)<br></code></pre></td></tr></table></figure><p>​b、优化器的梯度清零（因为梯度是会保存的）</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">optimizer.zero_grad<span class="hljs-comment">()</span><br></code></pre></td></tr></table></figure><p>​c、用误差函数计算预测值和实际值的误差</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">loss</span> = loss_functi<span class="hljs-literal">no</span>(model_prediction,labels)<br></code></pre></td></tr></table></figure><p>​d、计算梯度下降中每个参数的变化量</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">loss.backward<span class="hljs-comment">()</span><br></code></pre></td></tr></table></figure><p>​e、更新参数</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autoit">optimizer.<span class="hljs-keyword">step</span>()<br></code></pre></td></tr></table></figure><p>4、将string转化为tensor:</p><p>1）将数据集中的所有string按照字典序排序，然后依次将他们放入字典中，其中键值从1增加</p><p>2）每读入一条数据，依据字典将每个string转化为int,生成向量，并加入到张量tensor中。</p><p>3）因为tensor中每个数据包含的string的个数不同，所以需要填充</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.utils.rnn.pad_sequence(tensors,<span class="hljs-attribute">batch_first</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>batch_first&#x3D;True的作用是让张量的第一个维度是批量大小（也就是有几条数据）</p><p>5、embeddings:将每个token用向量表示，语义相近的token会聚集在一块。</p><p>6、lookup table:大小为[vocab_size,embedding_dim]的矩阵，也叫做查找表，用于存储每个token的embedding,方便查找获得</p>]]></content>
    
    
    
    <tags>
      
      <tag>make gpt from scratch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>gpt-day-2</title>
    <link href="/2024/05/22/gpt-day-2/"/>
    <url>/2024/05/22/gpt-day-2/</url>
    
    <content type="html"><![CDATA[<p>1、张量：有很多维度的数据集合，同时在 pytorch 中有很多派生属性</p><p>2、了解到了一些 pytorch 中常用的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br>a = torch.ones(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>)<br><br><span class="hljs-built_in">sum</span>=torch.<span class="hljs-built_in">sum</span>(a,axis=<span class="hljs-number">1</span>)<br>mean=torch.mean(a,axis=<span class="hljs-number">1</span>)<br><br><br><span class="hljs-comment">#降维和升维</span><br>x=torch.ones(<span class="hljs-number">5</span>,<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(x)<br>y=torch.squeeze(a)<br><span class="hljs-built_in">print</span>(y)<br><br>z=torch.unsqueeze(y,dim=<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br><br><span class="hljs-comment">#Constructor</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br><span class="hljs-built_in">super</span>().__init__()<br>self.first_layer = nn.Linear(<span class="hljs-number">4</span>,<span class="hljs-number">6</span>)<br><span class="hljs-comment">#之后对不同层初始化</span><br><br><span class="hljs-comment">#Forward // get_model_prediction(data)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>)<br><span class="hljs-comment">#如果想要得到第一层的结果</span><br>   first_layer_output=self.first_layer.forward(x)<br><br>t1=torch.reshape(x,(-<span class="hljs-number">1</span>,size)) <span class="hljs-comment">#-1的意思是会根据size自动计算</span><br>t2=torch.mean(x,axis=<span class="hljs-number">1</span>)<br>t3=torch.cat((x,y),dim=<span class="hljs-number">1</span>)<br>t4=torch.nn.functional.mse_loss(prediction,targer,reduction=<span class="hljs-string">&quot;mean&quot;</span>)<br>t5=torch.<span class="hljs-built_in">round</span>(x,decimals=<span class="hljs-number">5</span>) <span class="hljs-comment">#保留5位小数</span><br></code></pre></td></tr></table></figure><p>其中各个权重和偏差是按某种分布初始化的（我们无法看出）</p><p>3、过拟合出现的原因：</p><p>1）模型记住了太多不相关的细节（噪音</p><p>2）模型太过复杂</p><p>4、Dropout:一种用于解决过拟合的方法，让一些神经元以一定的概率被丢弃，不参与传播，这样可以降低模型的复杂度（但是不是永久删除，只是每次迭代中有一定概率被抛弃了）</p><p>5、激活函数：它使得神经元能够表示非线性变换，同时如果有输出或者大于一定阈值，为激活状态，能够传递信号；小于阈值或者为0，则是休眠状态。</p>]]></content>
    
    
    
    <tags>
      
      <tag>make gpt from scratch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>gpt-day-1</title>
    <link href="/2024/05/20/gpt-day-1/"/>
    <url>/2024/05/20/gpt-day-1/</url>
    
    <content type="html"><![CDATA[<!-- <blockquote class="imgur-embed-pub" lang="en" data-id="a/u28Owd3"  ><a href="//imgur.com/a/u28Owd3">first-try</a></blockquote><script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script> --><p>一、梯度下降法：在给定数据集，希望构建模型找出一般规律时，我们总是希望通过模型得到的数据和实际数据的误差尽可能小。我们可以把这个误差当做一个函数，并猜测误差最小的点为 init。为了使 init 尽可能靠近误差最小的点，我们需要计算该点的梯度，并设置学习率 learning_rate 和迭代次数 times,让 init 沿着梯度的反方向，运动 learning_rate 距离，经过多次迭代后会靠近函数极小点。</p><p>二、线性回归：</p><p>多元线性回归模型可以表示为：<br>y &#x3D; 𝛽1𝑋1 + 𝛽2𝑋2 + … + 𝛽𝑛𝑋𝑛 +b</p><p>其中，𝑌 是因变量，𝑋1,𝑋2,…,𝑋𝑛 是自变量，𝛽1,𝛽2,…,𝛽𝑛 是模型的参数，ε 是误差项，表示模型无法解释的部分。</p><p>线性回归的目标是找到最佳的参数，使得模型的预测值与实际观测值之间的残差（预测值与实际值之间的差异）最小化。这通常通过最小化残差平方和（即最小化平方损失函数）来实现，这个过程被称为最小二乘法。之所以用的是差异的平方，是因为取绝对值不好求导。</p><p>三、线性回归的实际应用：</p><p> 1）计算模型预测值 2）计算误差</p><p> 3）求导 4）更新参数</p><p> 以这个顺序迭代</p><p> 其中求导是通过计算出来的误差对该参数求导。因为误差使用的是均方误差，所以导数的形式近似于：</p><p> -2*（（实际数据矩阵-模拟数据矩阵）点乘（该参数对应的一列数据值））&#x2F;样本个数</p><p>四、神经网络：</p><p> 简单来说，就是一层层的线性回归，不断用一些参数线性回归产生新的参数，再用新的参数继续线性回归，直至得到最终的输出。</p>]]></content>
    
    
    
    <tags>
      
      <tag>make gpt from scratch</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
